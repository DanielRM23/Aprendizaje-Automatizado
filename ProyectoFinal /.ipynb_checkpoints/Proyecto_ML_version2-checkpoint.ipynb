{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5149c5f5",
   "metadata": {},
   "source": [
    "# **Ajustar Anchura** \n",
    "\n",
    "Esta línea hace que se ajuste la anchura del notebook, por defecto la ajusta a un 92%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6957203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container{ width:92% }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Permite ajustar la anchura de la parte útil de la libreta (reduce los márgenes)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container{ width:92% }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b492f5d8",
   "metadata": {},
   "source": [
    "# **Descargar Dependencias**\n",
    "\n",
    "Estos son los elementos que se tienen que descargar para un uso adecuado de todo el notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a303a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install swig\n",
    "# !pip install wrds\n",
    "# !pip install pyportfolioopt\n",
    "# !pip install git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
    "# !pip install yfinance\n",
    "# !pip install pandas_market_calendars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2958c73c",
   "metadata": {},
   "source": [
    "# **Se importan las librerías**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca975cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import yfinance as yf\n",
    "\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl import config_tickers\n",
    "from finrl.config import INDICATORS\n",
    "\n",
    "import itertools\n",
    "\n",
    "# 2. Separar en entrenamiento y prueba\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    f1_score\n",
    ")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1870083b",
   "metadata": {},
   "source": [
    "# **Se descargan los datos históricos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4300a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF deprecation warning: set proxy via new config function: yf.set_config(proxy=proxy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (1752, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANTE: Las fechas deben seguir el formato 'año/mes/día' (YYYY-MM-DD)\n",
    "# Estas fechas delimitan el periodo de tiempo del que se descargarán los datos históricos\n",
    "START_DATE = '2024-01-01'   # Fecha de inicio del análisis\n",
    "END_DATE = '2025-03-04'     # Fecha de fin del análisis\n",
    "\n",
    "# Lista de símbolos (tickers) de las acciones que se analizarán\n",
    "# Estos corresponden a empresas cotizadas en bolsa como Moderna, Nvidia, Uber, etc.\n",
    "symbols = [\n",
    "    \"MRNA\",  # Moderna Inc.\n",
    "    \"NVDA\",  # Nvidia Corp.\n",
    "    \"UBER\",  # Uber Technologies Inc.\n",
    "    \"ASML\",  # ASML Holding N.V.\n",
    "    \"AMZN\",  # Amazon.com Inc.\n",
    "    \"AAPL\"   # Apple Inc.\n",
    "]\n",
    "\n",
    "# Usamos el módulo YahooDownloader de FinRL para descargar datos históricos de acciones\n",
    "# Se especifica el rango de fechas y la lista de símbolos (acciones) definidos previamente\n",
    "data = YahooDownloader(\n",
    "    start_date = START_DATE,   # Fecha de inicio del periodo de análisis\n",
    "    end_date = END_DATE,       # Fecha de fin del periodo de análisis\n",
    "    ticker_list = symbols      # Lista de acciones a descargar (AAPL, AMZN, etc.)\n",
    ").fetch_data()                 # Ejecuta la descarga y devuelve un DataFrame con los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ef0da",
   "metadata": {},
   "source": [
    "# **Extracción de Indicadores Técnicos y Reconstrucción de la Estructura Temporal**\n",
    "\n",
    "### 1. Extracción de Indicadores Técnicos\n",
    "\n",
    "Después de descargar los datos históricos de precios para varias acciones, se aplica un proceso de **ingeniería de características** para enriquecer el conjunto de datos con variables útiles para el modelo de aprendizaje automatizado.\n",
    "\n",
    "Para esto, se utiliza el módulo `FeatureEngineer` de la biblioteca FinRL. Este módulo permite calcular automáticamente varios **indicadores técnicos**, que son ampliamente utilizados en el análisis técnico del mercado bursátil. Estos indicadores ayudan a capturar tendencias, momentum y señales de sobrecompra o sobreventa en los precios.\n",
    "\n",
    "Entre los indicadores extraídos se encuentran:\n",
    "\n",
    "- **RSI (Relative Strength Index)**\n",
    "- **MACD (Moving Average Convergence Divergence)**\n",
    "- **Bollinger Bands**\n",
    "- **Medias móviles (SMA, EMA)**\n",
    "- **CCI, DX, y más**\n",
    "\n",
    "Además, se incluyen variables adicionales como:\n",
    "\n",
    "- **VIX**: índice de volatilidad implícita del mercado, útil para medir el \"miedo\" del mercado.\n",
    "- **Turbulence**: una medida del comportamiento anómalo del mercado basada en desviaciones multivariadas.\n",
    "\n",
    "Estos indicadores se calculan para cada acción de forma individual y se agregan como nuevas columnas al DataFrame resultante (`processed`).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Reconstrucción de la estructura fecha × acción\n",
    "\n",
    "Una vez que se tienen los indicadores técnicos, se realiza un paso adicional: **reconstruir la estructura completa del conjunto de datos**, garantizando que todas las combinaciones posibles de fechas y acciones estén presentes.\n",
    "\n",
    "#### ¿Por qué se hace esto?\n",
    "\n",
    "En el mundo real, no todas las acciones tienen datos disponibles para todas las fechas (por ejemplo, por días festivos, suspensiones de cotización o errores en la descarga). Para asegurar que el conjunto de datos sea consistente y estructurado (especialmente útil para modelos temporales), se realiza lo siguiente:\n",
    "\n",
    "- Se genera una lista completa de fechas entre la mínima y máxima fecha observada.\n",
    "- Se toma la lista de acciones (tickers) presentes en el conjunto de datos.\n",
    "- Se calcula el **producto cartesiano** de fechas × acciones, creando todas las combinaciones posibles.\n",
    "- Este nuevo DataFrame se fusiona con los datos procesados originales para **rellenar los valores existentes** y dejar explícitos los faltantes.\n",
    "- Finalmente, se filtran las fechas que realmente ocurrieron en el mercado para evitar incluir días como fines de semana o festivos.\n",
    "\n",
    "Este paso garantiza que el conjunto de datos tenga una estructura rectangular y ordenada, lo cual es especialmente útil para la fase de modelado.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "901cf217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (291, 8)\n",
      "Successfully added vix\n",
      "Successfully added turbulence index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>vix</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>184.290405</td>\n",
       "      <td>187.070052</td>\n",
       "      <td>182.553128</td>\n",
       "      <td>185.789422</td>\n",
       "      <td>82488700.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>185.551913</td>\n",
       "      <td>181.649015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>184.290405</td>\n",
       "      <td>184.290405</td>\n",
       "      <td>13.20</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>182.910522</td>\n",
       "      <td>184.528677</td>\n",
       "      <td>182.096477</td>\n",
       "      <td>182.880742</td>\n",
       "      <td>58414500.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.030959</td>\n",
       "      <td>185.551913</td>\n",
       "      <td>181.649015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>183.600464</td>\n",
       "      <td>183.600464</td>\n",
       "      <td>14.04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>180.587555</td>\n",
       "      <td>181.758969</td>\n",
       "      <td>179.565044</td>\n",
       "      <td>180.825800</td>\n",
       "      <td>71983600.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.111483</td>\n",
       "      <td>186.338830</td>\n",
       "      <td>178.853492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>182.596161</td>\n",
       "      <td>182.596161</td>\n",
       "      <td>14.13</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>179.862839</td>\n",
       "      <td>181.431354</td>\n",
       "      <td>178.860187</td>\n",
       "      <td>180.666963</td>\n",
       "      <td>62303300.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.171540</td>\n",
       "      <td>186.012761</td>\n",
       "      <td>177.812900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-77.623425</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>181.912830</td>\n",
       "      <td>181.912830</td>\n",
       "      <td>13.35</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2024-01-08</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>184.210968</td>\n",
       "      <td>184.250686</td>\n",
       "      <td>180.180487</td>\n",
       "      <td>180.766194</td>\n",
       "      <td>59144500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.027541</td>\n",
       "      <td>186.475167</td>\n",
       "      <td>178.269749</td>\n",
       "      <td>51.361022</td>\n",
       "      <td>26.022128</td>\n",
       "      <td>7.073796</td>\n",
       "      <td>182.372458</td>\n",
       "      <td>182.372458</td>\n",
       "      <td>13.08</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date   tic       close        high         low        open  \\\n",
       "0   2024-01-02  AAPL  184.290405  187.070052  182.553128  185.789422   \n",
       "6   2024-01-03  AAPL  182.910522  184.528677  182.096477  182.880742   \n",
       "12  2024-01-04  AAPL  180.587555  181.758969  179.565044  180.825800   \n",
       "18  2024-01-05  AAPL  179.862839  181.431354  178.860187  180.666963   \n",
       "36  2024-01-08  AAPL  184.210968  184.250686  180.180487  180.766194   \n",
       "\n",
       "        volume  day      macd     boll_ub     boll_lb     rsi_30      cci_30  \\\n",
       "0   82488700.0  1.0  0.000000  185.551913  181.649015   0.000000  -66.666667   \n",
       "6   58414500.0  2.0 -0.030959  185.551913  181.649015   0.000000  -66.666667   \n",
       "12  71983600.0  3.0 -0.111483  186.338830  178.853492   0.000000 -100.000000   \n",
       "18  62303300.0  4.0 -0.171540  186.012761  177.812900   0.000000  -77.623425   \n",
       "36  59144500.0  0.0 -0.027541  186.475167  178.269749  51.361022   26.022128   \n",
       "\n",
       "         dx_30  close_30_sma  close_60_sma    vix  turbulence  \n",
       "0   100.000000    184.290405    184.290405  13.20         0.0  \n",
       "6   100.000000    183.600464    183.600464  14.04         0.0  \n",
       "12  100.000000    182.596161    182.596161  14.13         0.0  \n",
       "18  100.000000    181.912830    181.912830  13.35         0.0  \n",
       "36    7.073796    182.372458    182.372458  13.08         0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos un objeto FeatureEngineer para calcular automáticamente indicadores técnicos\n",
    "fe = FeatureEngineer(\n",
    "    use_technical_indicator=True,        # Activamos el cálculo de indicadores técnicos clásicos (RSI, MACD, etc.)\n",
    "    tech_indicator_list=INDICATORS,      # Usamos la lista predefinida de indicadores de FinRL\n",
    "    use_vix=True,                        # Incluye el índice VIX (volatilidad implícita del mercado)\n",
    "    use_turbulence=True,                 # Incluye la medida de turbulencia financiera\n",
    "    user_defined_feature=False           # No se agregan indicadores personalizados por ahora\n",
    ")\n",
    "\n",
    "# Aplicamos el preprocesamiento sobre el DataFrame descargado ('data') para generar nuevas columnas con indicadores\n",
    "processed = fe.preprocess_data(data)\n",
    "\n",
    "# --- Reconstruimos la estructura completa fecha × acción para evitar combinaciones faltantes ---\n",
    "\n",
    "# Obtenemos la lista única de tickers (acciones)\n",
    "list_ticker = processed[\"tic\"].unique().tolist()\n",
    "\n",
    "# Creamos una lista de fechas entre la mínima y máxima fecha disponibles en el dataset\n",
    "list_date = list(pd.date_range(processed['date'].min(), processed['date'].max()).astype(str))\n",
    "\n",
    "# Generamos todas las combinaciones posibles de (fecha, ticker)\n",
    "combination = list(itertools.product(list_date, list_ticker))\n",
    "\n",
    "# Creamos un nuevo DataFrame con todas las combinaciones posibles (fecha, acción)\n",
    "# Luego hacemos un left join con los datos procesados para rellenar los datos existentes\n",
    "processed_full = pd.DataFrame(combination, columns=[\"date\", \"tic\"]).merge(\n",
    "    processed, on=[\"date\", \"tic\"], how=\"left\"\n",
    ")\n",
    "\n",
    "# Filtramos para conservar solo las fechas que realmente estaban en los datos originales\n",
    "# Esto evita que aparezcan fechas inexistentes (por ejemplo, fines de semana o días festivos)\n",
    "processed_full = processed_full[processed_full['date'].isin(processed['date'])]\n",
    "\n",
    "# Ordenamos los datos primero por 'tic' (símbolo de la acción) y luego por 'date'\n",
    "# Esto es necesario para aplicar el rellenado hacia adelante (forward fill) correctamente dentro de cada acción\n",
    "processed_full = processed_full.sort_values(['tic', 'date'])\n",
    "\n",
    "# Rellenamos los valores faltantes con el último valor válido conocido hacia adelante (forward fill)\n",
    "# Esto es útil porque algunos indicadores técnicos no tienen valor en los primeros días y así evitamos NaNs\n",
    "processed_full = processed_full = processed_full.ffill()\n",
    "\n",
    "# Eliminamos cualquier fila que aún tenga valores faltantes después del rellenado\n",
    "# Esto suele ocurrir en los primeros días de cada acción, donde no hay valores previos para propagar\n",
    "processed_full = processed_full.dropna()\n",
    "\n",
    "\n",
    "#OPCIONAL: Visualizar la data\n",
    "processed_full.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685eaaab",
   "metadata": {},
   "source": [
    "# **Cálculo y Etiquetado de la Volatilidad**\n",
    "\n",
    "### Parte 1: Cálculo de la Volatilidad de 5 Días\n",
    "\n",
    "La volatilidad es una medida de qué tanto varían los precios de una acción en un periodo de tiempo. En este caso, la calculamos como la **desviación estándar de los rendimientos diarios** en una ventana móvil de 5 días.\n",
    "\n",
    "#### ¿Qué se hace?\n",
    "\n",
    "- Se agrupan los datos por acción (`tic`), ya que la volatilidad debe calcularse de forma independiente para cada activo.\n",
    "- Se aplica una **ventana móvil de 5 días** sobre la columna `return`, que representa el rendimiento diario.\n",
    "- Dentro de esa ventana, se calcula la **desviación estándar**, lo cual nos da una estimación local de la volatilidad.\n",
    "- El resultado se asigna como una nueva columna llamada `volatility_5d`.\n",
    "\n",
    "#### ¿Por qué usar desviación estándar?\n",
    "\n",
    "La desviación estándar es una medida clásica de **dispersión estadística**. Cuando los rendimientos de una acción fluctúan mucho en pocos días, la desviación estándar será alta. Por eso, se utiliza como una buena aproximación de la volatilidad en análisis financiero.\n",
    "\n",
    "---\n",
    "\n",
    "### Parte 2: Etiquetado de Días con \"Alta Volatilidad\"\n",
    "\n",
    "Para usar modelos de clasificación, necesitamos transformar la volatilidad continua en una variable binaria. Lo hacemos creando una etiqueta que indique si un día tiene o no **alta volatilidad**.\n",
    "\n",
    "#### ¿Qué se hace?\n",
    "\n",
    "- Se agrupan los datos por acción (`tic`), ya que cada acción puede tener un nivel típico de volatilidad distinto.\n",
    "- Se calcula el **percentil 75** (también llamado cuartil superior) de la columna `volatility_5d` para cada acción.\n",
    "- Este valor actúa como un **umbral dinámico**: representa qué tan volátil debe ser un día para ser considerado \"alto\" en el contexto de esa acción.\n",
    "- Para cada fila, se compara la volatilidad observada con ese umbral:\n",
    "  - Si la volatilidad es mayor al percentil 75 → se etiqueta como `1` (alta volatilidad).\n",
    "  - Si es menor o igual → se etiqueta como `0` (baja o normal volatilidad).\n",
    "- El resultado se almacena en una nueva columna llamada `volatilidad_alta`.\n",
    "\n",
    "---\n",
    "\n",
    "#### ¿Qué es el percentil 75 y por qué se usa?\n",
    "\n",
    "El percentil 75 es el valor por debajo del cual se encuentra el 75% de los datos. En este caso, representa un umbral de volatilidad \"alta\" relativo al comportamiento típico de cada acción. Si la volatilidad de un día supera este valor, se considera un evento inusualmente volátil. Esta estrategia permite adaptar el criterio de alta volatilidad a cada acción, en lugar de usar un valor fijo para todas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ccb3458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>...</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>vix</th>\n",
       "      <th>turbulence</th>\n",
       "      <th>return</th>\n",
       "      <th>volatility_5d</th>\n",
       "      <th>volatilidad_alta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2024-01-09</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>183.794067</td>\n",
       "      <td>183.803989</td>\n",
       "      <td>181.401584</td>\n",
       "      <td>182.582935</td>\n",
       "      <td>42841800.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.041713</td>\n",
       "      <td>186.458171</td>\n",
       "      <td>...</td>\n",
       "      <td>48.871354</td>\n",
       "      <td>29.331989</td>\n",
       "      <td>7.073796</td>\n",
       "      <td>182.609393</td>\n",
       "      <td>182.609393</td>\n",
       "      <td>12.760000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.002263</td>\n",
       "      <td>0.014335</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2024-01-10</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>184.836411</td>\n",
       "      <td>185.044875</td>\n",
       "      <td>182.582909</td>\n",
       "      <td>183.009791</td>\n",
       "      <td>46792900.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.132627</td>\n",
       "      <td>186.823473</td>\n",
       "      <td>...</td>\n",
       "      <td>54.567438</td>\n",
       "      <td>76.253898</td>\n",
       "      <td>13.207797</td>\n",
       "      <td>182.927538</td>\n",
       "      <td>182.927538</td>\n",
       "      <td>12.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005671</td>\n",
       "      <td>0.013924</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2024-01-11</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>184.240784</td>\n",
       "      <td>185.690176</td>\n",
       "      <td>182.285104</td>\n",
       "      <td>185.183874</td>\n",
       "      <td>49128400.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.163128</td>\n",
       "      <td>186.816245</td>\n",
       "      <td>...</td>\n",
       "      <td>51.195856</td>\n",
       "      <td>66.900628</td>\n",
       "      <td>21.476074</td>\n",
       "      <td>183.091694</td>\n",
       "      <td>183.091694</td>\n",
       "      <td>12.440000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.003222</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>184.568405</td>\n",
       "      <td>185.382451</td>\n",
       "      <td>183.843716</td>\n",
       "      <td>184.707387</td>\n",
       "      <td>40444700.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.198021</td>\n",
       "      <td>186.876192</td>\n",
       "      <td>...</td>\n",
       "      <td>52.853425</td>\n",
       "      <td>85.518864</td>\n",
       "      <td>21.476074</td>\n",
       "      <td>183.255773</td>\n",
       "      <td>183.255773</td>\n",
       "      <td>12.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001778</td>\n",
       "      <td>0.011165</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2024-01-16</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>182.295029</td>\n",
       "      <td>182.920438</td>\n",
       "      <td>179.614645</td>\n",
       "      <td>180.835714</td>\n",
       "      <td>65603000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.102793</td>\n",
       "      <td>186.626724</td>\n",
       "      <td>...</td>\n",
       "      <td>42.493441</td>\n",
       "      <td>-66.785087</td>\n",
       "      <td>29.103088</td>\n",
       "      <td>183.159698</td>\n",
       "      <td>183.159698</td>\n",
       "      <td>13.840000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.012317</td>\n",
       "      <td>0.006729</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2519</th>\n",
       "      <td>2025-02-24</td>\n",
       "      <td>UBER</td>\n",
       "      <td>76.419998</td>\n",
       "      <td>78.879997</td>\n",
       "      <td>74.849998</td>\n",
       "      <td>78.650002</td>\n",
       "      <td>24368400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.449048</td>\n",
       "      <td>86.061833</td>\n",
       "      <td>...</td>\n",
       "      <td>56.159591</td>\n",
       "      <td>65.676325</td>\n",
       "      <td>7.593468</td>\n",
       "      <td>71.628999</td>\n",
       "      <td>68.240166</td>\n",
       "      <td>18.980000</td>\n",
       "      <td>4.777462</td>\n",
       "      <td>-0.031309</td>\n",
       "      <td>0.023722</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2525</th>\n",
       "      <td>2025-02-25</td>\n",
       "      <td>UBER</td>\n",
       "      <td>74.949997</td>\n",
       "      <td>76.370003</td>\n",
       "      <td>73.529999</td>\n",
       "      <td>76.360001</td>\n",
       "      <td>19559200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.006083</td>\n",
       "      <td>86.140179</td>\n",
       "      <td>...</td>\n",
       "      <td>54.305930</td>\n",
       "      <td>39.701343</td>\n",
       "      <td>12.923284</td>\n",
       "      <td>71.928332</td>\n",
       "      <td>68.265666</td>\n",
       "      <td>19.430000</td>\n",
       "      <td>1.501328</td>\n",
       "      <td>-0.019236</td>\n",
       "      <td>0.014634</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2531</th>\n",
       "      <td>2025-02-26</td>\n",
       "      <td>UBER</td>\n",
       "      <td>75.870003</td>\n",
       "      <td>76.489998</td>\n",
       "      <td>75.309998</td>\n",
       "      <td>75.330002</td>\n",
       "      <td>10328900.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.698164</td>\n",
       "      <td>86.198854</td>\n",
       "      <td>...</td>\n",
       "      <td>55.261977</td>\n",
       "      <td>47.506516</td>\n",
       "      <td>12.293895</td>\n",
       "      <td>72.267332</td>\n",
       "      <td>68.337499</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>6.419840</td>\n",
       "      <td>0.012275</td>\n",
       "      <td>0.019213</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>2025-02-27</td>\n",
       "      <td>UBER</td>\n",
       "      <td>74.209999</td>\n",
       "      <td>77.690002</td>\n",
       "      <td>73.709999</td>\n",
       "      <td>75.949997</td>\n",
       "      <td>22535900.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.293746</td>\n",
       "      <td>85.983300</td>\n",
       "      <td>...</td>\n",
       "      <td>53.184914</td>\n",
       "      <td>35.376200</td>\n",
       "      <td>18.748735</td>\n",
       "      <td>72.579666</td>\n",
       "      <td>68.380666</td>\n",
       "      <td>21.129999</td>\n",
       "      <td>9.438051</td>\n",
       "      <td>-0.021880</td>\n",
       "      <td>0.017570</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2543</th>\n",
       "      <td>2025-02-28</td>\n",
       "      <td>UBER</td>\n",
       "      <td>76.010002</td>\n",
       "      <td>76.110001</td>\n",
       "      <td>73.580002</td>\n",
       "      <td>74.279999</td>\n",
       "      <td>17752000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.094345</td>\n",
       "      <td>85.739607</td>\n",
       "      <td>...</td>\n",
       "      <td>55.078834</td>\n",
       "      <td>32.740104</td>\n",
       "      <td>19.264791</td>\n",
       "      <td>72.879333</td>\n",
       "      <td>68.448166</td>\n",
       "      <td>19.629999</td>\n",
       "      <td>2.880587</td>\n",
       "      <td>0.024256</td>\n",
       "      <td>0.024033</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1716 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date   tic       close        high         low        open  \\\n",
       "42    2024-01-09  AAPL  183.794067  183.803989  181.401584  182.582935   \n",
       "48    2024-01-10  AAPL  184.836411  185.044875  182.582909  183.009791   \n",
       "54    2024-01-11  AAPL  184.240784  185.690176  182.285104  185.183874   \n",
       "60    2024-01-12  AAPL  184.568405  185.382451  183.843716  184.707387   \n",
       "84    2024-01-16  AAPL  182.295029  182.920438  179.614645  180.835714   \n",
       "...          ...   ...         ...         ...         ...         ...   \n",
       "2519  2025-02-24  UBER   76.419998   78.879997   74.849998   78.650002   \n",
       "2525  2025-02-25  UBER   74.949997   76.370003   73.529999   76.360001   \n",
       "2531  2025-02-26  UBER   75.870003   76.489998   75.309998   75.330002   \n",
       "2537  2025-02-27  UBER   74.209999   77.690002   73.709999   75.949997   \n",
       "2543  2025-02-28  UBER   76.010002   76.110001   73.580002   74.279999   \n",
       "\n",
       "          volume  day      macd     boll_ub  ...     rsi_30     cci_30  \\\n",
       "42    42841800.0  1.0  0.041713  186.458171  ...  48.871354  29.331989   \n",
       "48    46792900.0  2.0  0.132627  186.823473  ...  54.567438  76.253898   \n",
       "54    49128400.0  3.0  0.163128  186.816245  ...  51.195856  66.900628   \n",
       "60    40444700.0  4.0  0.198021  186.876192  ...  52.853425  85.518864   \n",
       "84    65603000.0  1.0  0.102793  186.626724  ...  42.493441 -66.785087   \n",
       "...          ...  ...       ...         ...  ...        ...        ...   \n",
       "2519  24368400.0  0.0  3.449048   86.061833  ...  56.159591  65.676325   \n",
       "2525  19559200.0  1.0  3.006083   86.140179  ...  54.305930  39.701343   \n",
       "2531  10328900.0  2.0  2.698164   86.198854  ...  55.261977  47.506516   \n",
       "2537  22535900.0  3.0  2.293746   85.983300  ...  53.184914  35.376200   \n",
       "2543  17752000.0  4.0  2.094345   85.739607  ...  55.078834  32.740104   \n",
       "\n",
       "          dx_30  close_30_sma  close_60_sma        vix  turbulence    return  \\\n",
       "42     7.073796    182.609393    182.609393  12.760000    0.000000 -0.002263   \n",
       "48    13.207797    182.927538    182.927538  12.690000    0.000000  0.005671   \n",
       "54    21.476074    183.091694    183.091694  12.440000    0.000000 -0.003222   \n",
       "60    21.476074    183.255773    183.255773  12.700000    0.000000  0.001778   \n",
       "84    29.103088    183.159698    183.159698  13.840000    0.000000 -0.012317   \n",
       "...         ...           ...           ...        ...         ...       ...   \n",
       "2519   7.593468     71.628999     68.240166  18.980000    4.777462 -0.031309   \n",
       "2525  12.923284     71.928332     68.265666  19.430000    1.501328 -0.019236   \n",
       "2531  12.293895     72.267332     68.337499  19.100000    6.419840  0.012275   \n",
       "2537  18.748735     72.579666     68.380666  21.129999    9.438051 -0.021880   \n",
       "2543  19.264791     72.879333     68.448166  19.629999    2.880587  0.024256   \n",
       "\n",
       "      volatility_5d  volatilidad_alta  \n",
       "42         0.014335                 0  \n",
       "48         0.013924                 0  \n",
       "54         0.011889                 0  \n",
       "60         0.011165                 0  \n",
       "84         0.006729                 0  \n",
       "...             ...               ...  \n",
       "2519       0.023722                 0  \n",
       "2525       0.014634                 0  \n",
       "2531       0.019213                 0  \n",
       "2537       0.017570                 0  \n",
       "2543       0.024033                 0  \n",
       "\n",
       "[1716 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Ordenamos por acción y fecha\n",
    "processed_full = processed_full.sort_values(['tic', 'date'])\n",
    "\n",
    "# 2. Calculamos el rendimiento diario por acción\n",
    "processed_full['return'] = processed_full.groupby('tic')['close'].pct_change()\n",
    "\n",
    "# 3. Calculamos la volatilidad como desviación estándar de 5 días sobre los rendimientos\n",
    "processed_full['volatility_5d'] = processed_full.groupby('tic')['return'].rolling(5).std().reset_index(0, drop=True)\n",
    "\n",
    "# 4. Etiquetamos los días con volatilidad alta (top 25% por acción)\n",
    "def etiquetar_volatilidad(df, column='volatility_5d'):\n",
    "    umbrales = df.groupby('tic')[column].transform(lambda x: x.quantile(0.75))\n",
    "    df['volatilidad_alta'] = (df[column] > umbrales).astype(int)\n",
    "    return df\n",
    "\n",
    "processed_full = etiquetar_volatilidad(processed_full)\n",
    "\n",
    "# 5. Eliminamos filas con valores faltantes\n",
    "processed_full = processed_full.dropna()\n",
    "\n",
    "processed_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afc8de3",
   "metadata": {},
   "source": [
    "# **Descripción de las columnas del dataset final (`data_para_modelo.csv`)**\n",
    "\n",
    "A continuación se describen brevemente las columnas del conjunto de datos que se utilizará para entrenar los modelos de clasificación:\n",
    "\n",
    "- **close**: Precio de cierre de la acción en el día correspondiente.\n",
    "- **high**: Precio más alto alcanzado por la acción durante el día.\n",
    "- **low**: Precio más bajo alcanzado por la acción durante el día.\n",
    "- **open**: Precio de apertura de la acción en ese día.\n",
    "- **volume**: Volumen de operaciones (cantidad de acciones intercambiadas en el día).\n",
    "- **day**: Día de la semana representado como número (0 = lunes, 6 = domingo).\n",
    "\n",
    "### Indicadores técnicos (features extraídas automáticamente):\n",
    "- **macd**: Media móvil de convergencia/divergencia, indicador de momentum.\n",
    "- **boll_ub** / **boll_lb**: Bandas de Bollinger superior e inferior, usadas para detectar sobrecompra o sobreventa.\n",
    "- **rsi_30**: Índice de fuerza relativa (RSI) con ventana de 30 días.\n",
    "- **cci_30**: Commodity Channel Index, mide la variación del precio respecto a su media.\n",
    "- **dx_30**: Directional Movement Index, evalúa la fuerza de una tendencia.\n",
    "- **close_30_sma** / **close_60_sma**: Medias móviles simples del precio de cierre en ventanas de 30 y 60 días.\n",
    "\n",
    "### Etiqueta (target):\n",
    "- **volatilidad_alta**: Variable binaria que indica si el día fue clasificado como de alta volatilidad (`1`) o no (`0`), calculado con base en el percentil 75 de la volatilidad histórica por acción.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c38566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcional: inspeccionar columnas\n",
    "print(processed_full.columns)\n",
    "\n",
    "# Definir columnas a excluir\n",
    "columnas_excluir = ['date', 'tic', 'return', 'volatility_5d']\n",
    "\n",
    "# Crear DataFrame solo con features + target\n",
    "df_model = processed_full.drop(columns=columnas_excluir, errors='ignore')\n",
    "\n",
    "# Acomodar columnas para dejar 'volatilidad_alta' al final (opcional, por claridad)\n",
    "columnas = [col for col in df_model.columns if col != 'volatilidad_alta'] + ['volatilidad_alta']\n",
    "df_model = df_model[columnas]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfed6860",
   "metadata": {},
   "source": [
    "## **Opcional**\n",
    "\n",
    "Se guarda la data ya preprocesada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92a8afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_model.to_csv(\"data_para_modelo.csv\", index=False)\n",
    "# print(\"Archivo 'data_para_modelo.csv' guardado correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae934d9",
   "metadata": {},
   "source": [
    "# **Análisis de la Distribución de Clases**\n",
    "\n",
    "Antes de entrenar cualquier modelo, es importante revisar cuántos ejemplos pertenecen a cada clase en la variable objetivo `volatilidad_alta`. Para esto, se contabilizó el número y el porcentaje de ejemplos con:\n",
    "\n",
    "- `0`: días de baja o normal volatilidad\n",
    "- `1`: días de alta volatilidad\n",
    "\n",
    "### ¿Por qué tiene sentido?\n",
    "\n",
    "La etiqueta `volatilidad_alta` se construyó usando el **percentil 75** de la volatilidad histórica de cada acción. Esto significa que, por diseño, **aproximadamente el 25%** de los días más volátiles fueron etiquetados como `1` (alta volatilidad), y el **75% restante** como `0` (normal o baja volatilidad).\n",
    "\n",
    "Esto explica que la distribución de clases no sea balanceada al 50%, sino más cercana a una proporción 75/25, lo cual es esperable y coherente con el criterio de etiquetado aplicado.\n",
    "\n",
    "Esta revisión también permite tomar decisiones informadas sobre métricas de evaluación (como F1-score) y, si fuera necesario, aplicar técnicas de balanceo o ajuste de pesos durante el entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bc6a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Separar X (características) e y (etiqueta)\n",
    "# Excluimos la columna 'volatilidad_alta' de X porque es la variable que queremos predecir\n",
    "X = df_model.drop(columns=['volatilidad_alta'])\n",
    "y = df_model['volatilidad_alta']\n",
    "\n",
    "# 2. Dividir el conjunto en entrenamiento (80%) y prueba (20%)\n",
    "# Usamos shuffle=False porque los datos son temporales (series de tiempo) y no deben mezclarse\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# 3. Escalar las características\n",
    "# Aplicamos StandardScaler para llevar todas las columnas numéricas a media 0 y desviación estándar 1\n",
    "# Ajustamos el escalador con los datos de entrenamiento y luego transformamos ambos conjuntos\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Reconstruir el DataFrame escalado completo (entrenamiento + prueba)\n",
    "# Unimos ambos arrays escalados y los convertimos a un DataFrame con los mismos nombres de columnas originales\n",
    "X_scaled = pd.DataFrame(\n",
    "    np.concatenate([X_train_scaled, X_test_scaled]),\n",
    "    columns=X.columns\n",
    ")\n",
    "\n",
    "# 5. Añadir la columna de etiqueta (volatilidad alta) al DataFrame escalado\n",
    "# Concatenamos las etiquetas originales (y_train + y_test) y las alineamos con los datos escalados\n",
    "X_scaled['volatilidad_alta'] = pd.concat([y_train, y_test]).reset_index(drop=True)\n",
    "\n",
    "# 6. OPCIONAL: Guardar el DataFrame completo escalado como CSV\n",
    "# X_scaled.to_csv(\"data_escalada.csv\", index=False)\n",
    "# print(\"Archivo 'data_escalada.csv' guardado correctamente.\")\n",
    "\n",
    "print(\"-\"*50)\n",
    "# Contar cuántas veces aparece cada clase (0 o 1) en la columna 'volatilidad_alta'\n",
    "# Esto nos dice cuántos ejemplos hay de días con baja/normal volatilidad (0) y de alta volatilidad (1)\n",
    "conteo_clases = X_scaled['volatilidad_alta'].value_counts()\n",
    "\n",
    "# Mostramos el número total de ejemplos por clase\n",
    "print(conteo_clases)\n",
    "\n",
    "print(\"-\"*50)\n",
    "# Calculamos el porcentaje que representa cada clase respecto al total\n",
    "# normalize=True hace que los resultados estén entre 0 y 1, y luego se multiplican por 100 para expresarlos como porcentaje\n",
    "porcentaje_clases = X_scaled['volatilidad_alta'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Mostramos el porcentaje de cada clase (útil para entender el balance del dataset)\n",
    "print(porcentaje_clases)\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f5050a",
   "metadata": {},
   "source": [
    "# Modelos Clásicos de Clasificación Evaluados\n",
    "\n",
    "Para clasificar los días con alta volatilidad se probaron cinco modelos clásicos de aprendizaje supervisado, cada uno representando un enfoque distinto:\n",
    "\n",
    "- **Random Forest**: Ensamble de árboles, robusto y eficaz con datos tabulares. Resiste bien el sobreajuste.\n",
    "\n",
    "- **Regresión Logística**: Modelo lineal e interpretable, usado como *benchmark* básico.\n",
    "\n",
    "- **XGBoost**: Técnica avanzada de *boosting*, capaz de capturar relaciones no lineales y manejar desbalanceo.\n",
    "\n",
    "- **SVM**: Modelo que busca la mejor frontera entre clases; útil con *kernels* para capturar relaciones complejas.\n",
    "\n",
    "- **KNN**: Clasificador basado en distancia, simple pero útil como contraste frente a modelos más estructurados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "308332f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_modelo(modelo, nombre, X_train, y_train, X_test, y_test, mostrar_roc=True):\n",
    "    \"\"\"\n",
    "    Entrena y evalúa un modelo de clasificación.\n",
    "\n",
    "    Retorna:\n",
    "        accuracy, f1_score, auc (si aplica, si no retorna None)\n",
    "    \"\"\"\n",
    "    # Entrenamiento\n",
    "    modelo.fit(X_train, y_train)\n",
    "    y_pred = modelo.predict(X_test)\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # F1-score para la clase 1 (alta volatilidad)\n",
    "    f1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "\n",
    "    # AUC (solo si el modelo tiene predict_proba y se solicita)\n",
    "    auc = None\n",
    "    if mostrar_roc and hasattr(modelo, \"predict_proba\"):\n",
    "        y_proba = modelo.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    return acc, f1, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e0031d",
   "metadata": {},
   "source": [
    "# Motivación de los Indicadores Utilizados para el Entrenamiento\n",
    "\n",
    "En este trabajo buscamos predecir si un día determinado tendrá alta volatilidad en el precio de una acción. Para ello, seleccionamos diferentes grupos de variables que capturan distintos aspectos del comportamiento del mercado. A continuación se justifica cada grupo evaluado:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Precios del activo (`open`, `close`, `high`, `low`)\n",
    "\n",
    "Los precios reflejan la información más inmediata del mercado:\n",
    "\n",
    "- `open` y `close` representan el precio inicial y final del día.\n",
    "- `high` y `low` indican los extremos de volatilidad intradía.\n",
    "\n",
    "Estos datos permiten detectar movimientos bruscos en precios que pueden correlacionarse con futuros periodos de alta volatilidad.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Indicadores técnicos\n",
    "\n",
    "Los indicadores técnicos como `RSI`, `MACD`, `CCI`, `Bollinger Bands`, `DX`, etc., son comúnmente usados en el análisis técnico para detectar:\n",
    "\n",
    "- Tendencias (por ejemplo, `MACD`),\n",
    "- Niveles de sobrecompra o sobreventa (por ejemplo, `RSI`, `CCI`),\n",
    "- Cambios de dirección del mercado.\n",
    "\n",
    "Estos indicadores agregan una capa de interpretación sobre los precios, ayudando a anticipar periodos de alta volatilidad.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Indicadores de riesgo de mercado (`VIX`, `turbulence`)\n",
    "\n",
    "- `VIX` es un índice que mide la volatilidad implícita esperada del mercado. Es un indicador reconocido de “miedo” en los inversionistas.\n",
    "- `turbulence` mide la inestabilidad o comportamiento anómalo del mercado comparado con su comportamiento histórico.\n",
    "\n",
    "Ambos son indicadores exógenos que pueden influir fuertemente en la volatilidad individual de una acción.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Evaluación de todas las combinaciones\n",
    "\n",
    "Para comprender el aporte predictivo de cada grupo de variables, se evaluaron las siguientes configuraciones:\n",
    "\n",
    "| Combinación de variables     | Descripción                                                                 |\n",
    "|------------------------------|-----------------------------------------------------------------------------|\n",
    "| Todas las variables          | Incluye precios, indicadores técnicos y de riesgo                          |\n",
    "| Solo indicadores técnicos    | MACD, RSI, CCI, Bollinger Bands, DX, medias móviles                        |\n",
    "| Solo precios                 | Precios diarios: `open`, `close`, `high`, `low`                            |\n",
    "| Solo VIX + Turbulence        | Indicadores exógenos de riesgo e inestabilidad del mercado                 |\n",
    "\n",
    "Estas combinaciones se entrenaron y evaluaron utilizando cinco modelos de clasificación distintos para obtener una comparación robusta.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Selección Evolutiva de Características\n",
    "\n",
    "Para optimizar aún más el rendimiento del modelo, se implementó una estrategia basada en un **Algoritmo Genético (GA)** con el fin de seleccionar automáticamente un subconjunto óptimo de características entre todas las disponibles.\n",
    "\n",
    "Esta estrategia permitió:\n",
    "\n",
    "- Explorar de forma inteligente el espacio de combinaciones posibles.\n",
    "- Combinar buenas soluciones parciales mediante operadores de cruce y mutación.\n",
    "- Identificar subconjuntos de variables que maximizan la precisión promedio en múltiples clasificadores (Random Forest, Regresión Logística, XGBoost, SVM y KNN).\n",
    "\n",
    "La selección evolutiva no solo ayudó a reducir la dimensionalidad del problema, sino que también mejoró el desempeño en algunos casos respecto a configuraciones manuales, y permitió establecer una base sólida para el entrenamiento de modelos más complejos como las redes neuronales recurrentes (RNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857c6aea",
   "metadata": {},
   "source": [
    "### Partición del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ebbd295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# =========================\n",
    "# Helper para aplicar One-Hot a 'day'\n",
    "# =========================\n",
    "def aplicar_one_hot(df, incluir_day=True):\n",
    "    df = df.copy()\n",
    "    if incluir_day and 'day' in df.columns:\n",
    "        df = pd.get_dummies(df, columns=['day'], prefix='day')\n",
    "    return df\n",
    "\n",
    "# =========================\n",
    "# Todas las variables (con 'day' en One-Hot)\n",
    "# =========================\n",
    "columnas_excluir = ['date', 'tic', 'volatilidad_alta']\n",
    "columnas_modelo = [col for col in processed_full.columns if col not in columnas_excluir]\n",
    "\n",
    "X_todas = processed_full[columnas_modelo].copy()\n",
    "X_todas = aplicar_one_hot(X_todas)  # One-Hot para 'day'\n",
    "y_todas = processed_full['volatilidad_alta']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_todas, y_todas, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "scaler_todas = StandardScaler()\n",
    "X_train_scaled = scaler_todas.fit_transform(X_train)\n",
    "X_test_scaled = scaler_todas.transform(X_test)\n",
    "\n",
    "# =========================\n",
    "# Indicadores Técnicos (con 'day')\n",
    "# =========================\n",
    "columnas_indicadores = [\n",
    "    \"macd\", \"boll_ub\", \"boll_lb\", \"rsi_30\", \"cci_30\", \"dx_30\",\n",
    "    \"close_30_sma\", \"close_60_sma\", \"day\"  # incluimos explícitamente 'day'\n",
    "]\n",
    "df_indicadores = processed_full[columnas_indicadores + [\"volatilidad_alta\"]]\n",
    "df_indicadores = aplicar_one_hot(df_indicadores)\n",
    "\n",
    "X_indicadores = df_indicadores.drop(columns=['volatilidad_alta'])\n",
    "y_indicadores = df_indicadores['volatilidad_alta']\n",
    "\n",
    "X_train_ind, X_test_ind, y_train_ind, y_test_ind = train_test_split(\n",
    "    X_indicadores, y_indicadores, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "scaler_ind = StandardScaler()\n",
    "X_train_ind_scaled = scaler_ind.fit_transform(X_train_ind)\n",
    "X_test_ind_scaled = scaler_ind.transform(X_test_ind)\n",
    "\n",
    "# =========================\n",
    "# Solo Precios (con 'day')\n",
    "# =========================\n",
    "columnas_precios = ['open', 'close', 'high', 'low', 'day']\n",
    "df_precios = processed_full[columnas_precios + ['volatilidad_alta']]\n",
    "df_precios = aplicar_one_hot(df_precios)\n",
    "\n",
    "X_precios = df_precios.drop(columns=['volatilidad_alta'])\n",
    "y_precios = df_precios['volatilidad_alta']\n",
    "\n",
    "X_train_precios, X_test_precios, y_train_precios, y_test_precios = train_test_split(\n",
    "    X_precios, y_precios, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "scaler_prec = StandardScaler()\n",
    "X_train_precios_scaled = scaler_prec.fit_transform(X_train_precios)\n",
    "X_test_precios_scaled = scaler_prec.transform(X_test_precios)\n",
    "\n",
    "# =========================\n",
    "# VIX + Turbulence (con 'day')\n",
    "# =========================\n",
    "columnas_vix_turb = ['vix', 'turbulence', 'day']\n",
    "df_vix_turbulence = processed_full[columnas_vix_turb + ['volatilidad_alta']]\n",
    "df_vix_turbulence = aplicar_one_hot(df_vix_turbulence)\n",
    "\n",
    "X_vix_turbulence = df_vix_turbulence.drop(columns=['volatilidad_alta'])\n",
    "y_vix_turbulence = df_vix_turbulence['volatilidad_alta']\n",
    "\n",
    "X_train_vix_turb, X_test_vix_turb, y_train_vix_turb, y_test_vix_turb = train_test_split(\n",
    "    X_vix_turbulence, y_vix_turbulence, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "scaler_vix_turb = StandardScaler()\n",
    "X_train_vix_turb_scaled = scaler_vix_turb.fit_transform(X_train_vix_turb)\n",
    "X_test_vix_turb_scaled = scaler_vix_turb.transform(X_test_vix_turb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3ded54",
   "metadata": {},
   "source": [
    "### Evaluación de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "394409bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión promedio (Todas las variables): 0.8715\n",
      "Precisión promedio (Indicadores técnicos): 0.7372\n",
      "Precisión promedio (Solo precios): 0.7395\n",
      "Precisión promedio (VIX + Turbulence): 0.7390\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# DEFINICIÓN DE CLASIFICADORES A EVALUAR\n",
    "# ===============================================================\n",
    "\n",
    "# Diccionario con cinco modelos clásicos de clasificación supervisada.\n",
    "# Cada entrada tiene un nombre (para identificación en resultados) y su instancia.\n",
    "clasificadores = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Regresión Logística\": LogisticRegression(max_iter=500),\n",
    "    \"XGBoost\": XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "    \"SVM\": SVC(probability=True, random_state=42),  # Se habilita probability para AUC\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# ===============================================================\n",
    "# CONJUNTOS DE DATOS A PROBAR\n",
    "# ===============================================================\n",
    "\n",
    "# Lista de diccionarios, cada uno representa una combinación de variables.\n",
    "# Cada combinación contiene sus datos de entrenamiento y prueba (ya escalados).\n",
    "conjuntos = [\n",
    "    {\n",
    "        \"nombre\": \"Todas las variables\",\n",
    "        \"X_train\": X_train_scaled,\n",
    "        \"X_test\": X_test_scaled,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_test\": y_test\n",
    "    },\n",
    "    {\n",
    "        \"nombre\": \"Indicadores técnicos\",\n",
    "        \"X_train\": X_train_ind_scaled,\n",
    "        \"X_test\": X_test_ind_scaled,\n",
    "        \"y_train\": y_train_ind,\n",
    "        \"y_test\": y_test_ind\n",
    "    },\n",
    "    {\n",
    "        \"nombre\": \"Solo precios\",\n",
    "        \"X_train\": X_train_precios_scaled,\n",
    "        \"X_test\": X_test_precios_scaled,\n",
    "        \"y_train\": y_train_precios,\n",
    "        \"y_test\": y_test_precios\n",
    "    },\n",
    "    {\n",
    "        \"nombre\": \"VIX + Turbulence\",\n",
    "        \"X_train\": X_train_vix_turb_scaled,\n",
    "        \"X_test\": X_test_vix_turb_scaled,\n",
    "        \"y_train\": y_train_vix_turb,\n",
    "        \"y_test\": y_test_vix_turb\n",
    "    }\n",
    "]\n",
    "\n",
    "# ===============================================================\n",
    "# EVALUACIÓN DE CADA MODELO SOBRE CADA CONJUNTO DE VARIABLES\n",
    "# ===============================================================\n",
    "\n",
    "# Lista donde se almacenarán los resultados promedio por combinación\n",
    "resultados = []\n",
    "\n",
    "# Iteramos sobre cada conjunto definido (por nombre y datos)\n",
    "for conjunto in conjuntos:\n",
    "    nombre_combo = conjunto[\"nombre\"]\n",
    "    X_train = conjunto[\"X_train\"]\n",
    "    X_test = conjunto[\"X_test\"]\n",
    "    y_train = conjunto[\"y_train\"]\n",
    "    y_test = conjunto[\"y_test\"]\n",
    "\n",
    "    # Lista para almacenar accuracy de cada modelo en esta combinación\n",
    "    accuracies = []\n",
    "\n",
    "    # Evaluamos cada modelo con la función definida previamente\n",
    "    for nombre_modelo, modelo in clasificadores.items():\n",
    "        acc, _, _ = evaluar_modelo(\n",
    "            modelo,\n",
    "            f\"{nombre_modelo} ({nombre_combo})\",\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            y_test\n",
    "        )\n",
    "        accuracies.append(acc)\n",
    "\n",
    "    # Promediamos las precisiones de todos los modelos sobre esta combinación\n",
    "    promedio = np.mean(accuracies)\n",
    "\n",
    "    # Guardamos el resultado\n",
    "    resultados.append({\n",
    "        \"nombre\": nombre_combo,\n",
    "        \"promedio\": promedio\n",
    "    })\n",
    "\n",
    "    # Mostramos el resultado por consola\n",
    "    print(f\"Precisión promedio ({nombre_combo}): {promedio:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa88444e",
   "metadata": {},
   "source": [
    "# Estrategia Evolutiva\n",
    "\n",
    "\n",
    "Con el objetivo de mejorar la precisión en la clasificación de días con alta volatilidad, se implementó una estrategia basada en un Algoritmo Genético (GA) para seleccionar automáticamente el subconjunto óptimo de características entre todas las disponibles.\n",
    "\n",
    "---\n",
    "\n",
    "### Motivación\n",
    "\n",
    "El número de posibles combinaciones de columnas crece exponencialmente, y probarlas manualmente no es factible. Además, no todas las variables aportan valor predictivo; algunas pueden introducir ruido o redundancia. El algoritmo genético permite:\n",
    "\n",
    "- Explorar inteligentemente el espacio de búsqueda.\n",
    "- Combinar buenas soluciones parciales.\n",
    "- Identificar subconjuntos que maximizan la precisión promedio en múltiples modelos.\n",
    "\n",
    "---\n",
    "\n",
    "### Estrategia Evolutiva\n",
    "\n",
    "#### 1. Codificación de Individuos\n",
    "\n",
    "Cada individuo del algoritmo representa un subconjunto de columnas codificado como un vector binario:\n",
    "\n",
    "- `1`: la columna está incluida.\n",
    "- `0`: la columna está descartada.\n",
    "\n",
    "#### 2. Función de Aptitud (Fitness)\n",
    "\n",
    "Para cada subconjunto (individuo), se evalúa su rendimiento promedio usando cinco clasificadores:\n",
    "\n",
    "- Random Forest\n",
    "- Regresión Logística\n",
    "- XGBoost\n",
    "- SVM\n",
    "- K-Nearest Neighbors\n",
    "\n",
    "El valor de fitness es el promedio de *accuracy* entre estos modelos, evaluados en un conjunto de prueba fijo.\n",
    "\n",
    "#### 3. Parámetros Utilizados\n",
    "\n",
    "| Parámetro             | Valor                         |\n",
    "|-----------------------|-------------------------------|\n",
    "| Tamaño de población   | 10 a 40 individuos            |\n",
    "| Número de generaciones| 5 a 30 generaciones           |\n",
    "| Tasa de mutación      | 0.1 (10%)                     |\n",
    "| Selección             | Mejores 50% por generación    |\n",
    "| Cruce (crossover)     | 1 punto                       |\n",
    "| Mutación              | Cambio de bits aleatorio      |\n",
    "\n",
    "#### 4. Resultados\n",
    "\n",
    "El algoritmo evolutivo fue capaz de:\n",
    "\n",
    "- Identificar subconjuntos pequeños con buen rendimiento.\n",
    "- Superar algunas configuraciones manuales.\n",
    "- Adaptarse al conjunto completo de columnas, descartando automáticamente aquellas menos útiles.\n",
    "\n",
    "---\n",
    "\n",
    "### Ventajas\n",
    "\n",
    "- Automatiza la búsqueda del mejor subconjunto de variables.\n",
    "- Considera múltiples modelos simultáneamente.\n",
    "- Reduce el riesgo de *overfitting* por selección manual.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dec8430f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generación 1: Mejor accuracy = 0.7488\n",
      "Generación 2: Mejor accuracy = 0.7488\n",
      "Generación 3: Mejor accuracy = 0.7558\n",
      "Generación 4: Mejor accuracy = 0.7576\n",
      "Generación 5: Mejor accuracy = 0.7576\n",
      "Generación 6: Mejor accuracy = 0.7622\n",
      "Generación 7: Mejor accuracy = 0.7622\n",
      "Generación 8: Mejor accuracy = 0.7622\n",
      "Generación 9: Mejor accuracy = 0.7622\n",
      "Generación 10: Mejor accuracy = 0.7622\n",
      "Generación 11: Mejor accuracy = 0.7715\n",
      "Generación 12: Mejor accuracy = 0.7715\n",
      "Generación 13: Mejor accuracy = 0.7715\n",
      "Generación 14: Mejor accuracy = 0.7715\n",
      "Generación 15: Mejor accuracy = 0.7721\n",
      "Generación 16: Mejor accuracy = 0.7721\n",
      "Generación 17: Mejor accuracy = 0.7721\n",
      "Generación 18: Mejor accuracy = 0.7738\n",
      "Generación 19: Mejor accuracy = 0.7738\n",
      "Generación 20: Mejor accuracy = 0.7738\n",
      "Generación 21: Mejor accuracy = 0.7738\n",
      "Generación 22: Mejor accuracy = 0.7738\n",
      "Generación 23: Mejor accuracy = 0.7738\n",
      "Generación 24: Mejor accuracy = 0.7738\n",
      "Generación 25: Mejor accuracy = 0.7738\n",
      "Generación 26: Mejor accuracy = 0.7738\n",
      "Generación 27: Mejor accuracy = 0.7738\n",
      "Generación 28: Mejor accuracy = 0.7738\n",
      "Generación 29: Mejor accuracy = 0.7744\n",
      "Generación 30: Mejor accuracy = 0.7779\n",
      "\n",
      " Mejor Accuracy Promedio GA: 0.7779069767441861\n",
      " Columnas Seleccionadas: ['volume', 'boll_ub', 'cci_30', 'dx_30', 'close_60_sma', 'day_2.0', 'day_4.0']\n"
     ]
    }
   ],
   "source": [
    "# 1) Definir df_model\n",
    "df_model = processed_full.drop(columns=['date', 'tic', 'return', 'volatility_5d'])\n",
    "\n",
    "# Aplicar One-Hot Encoding a 'day' si existe\n",
    "if 'day' in df_model.columns:\n",
    "    df_model = pd.get_dummies(df_model, columns=['day'], prefix='day')\n",
    "\n",
    "# Asegurar que 'volatilidad_alta' esté al final\n",
    "cols = [c for c in df_model.columns if c != 'volatilidad_alta'] + ['volatilidad_alta']\n",
    "df_model = df_model[cols]\n",
    "\n",
    "# 2) Separar X e y\n",
    "X = df_model.drop(columns=['volatilidad_alta'])\n",
    "y = df_model['volatilidad_alta']\n",
    "\n",
    "# 3) División temporal\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# 4) Escalado\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 5) Parámetros del GA\n",
    "POP_SIZE = 10\n",
    "N_GENERATIONS = 30\n",
    "MUTATION_RATE = 0.1\n",
    "N_FEATURES = X.shape[1]\n",
    "\n",
    "# 6) Creación, mutación y cruce\n",
    "def create_individual():\n",
    "    return np.random.choice([0, 1], size=N_FEATURES)\n",
    "\n",
    "def mutate(ind):\n",
    "    m = ind.copy()\n",
    "    for i in range(N_FEATURES):\n",
    "        if np.random.rand() < MUTATION_RATE:\n",
    "            m[i] = 1 - m[i]\n",
    "    return m\n",
    "\n",
    "def crossover(p1, p2):\n",
    "    if N_FEATURES < 3:\n",
    "        return p1.copy(), p2.copy()\n",
    "    pt = np.random.randint(1, N_FEATURES-1)\n",
    "    return (\n",
    "        np.concatenate([p1[:pt], p2[pt:]]),\n",
    "        np.concatenate([p2[:pt], p1[pt:]])\n",
    "    )\n",
    "\n",
    "# 7) Función de fitness\n",
    "def fitness(ind):\n",
    "    sel = np.where(ind == 1)[0]\n",
    "    if len(sel) == 0:\n",
    "        return 0\n",
    "    Xtr, Xte = X_train_scaled[:, sel], X_test_scaled[:, sel]\n",
    "    models = [\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        LogisticRegression(max_iter=500),\n",
    "        XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        SVC(probability=True, random_state=42),\n",
    "        KNeighborsClassifier()\n",
    "    ]\n",
    "    scores = []\n",
    "    for m in models:\n",
    "        try:\n",
    "            m.fit(Xtr, y_train)\n",
    "            scores.append(m.score(Xte, y_test))\n",
    "        except:\n",
    "            scores.append(0)\n",
    "    return np.mean(scores)\n",
    "\n",
    "# 8) Inicializar población\n",
    "pop = [create_individual() for _ in range(POP_SIZE)]\n",
    "\n",
    "# 9) Evolución\n",
    "for gen in range(N_GENERATIONS):\n",
    "    fits = [fitness(ind) for ind in pop]\n",
    "    ranked = sorted(zip(pop, fits), key=lambda x: x[1], reverse=True)\n",
    "    survivors = [ind for ind, _ in ranked[:POP_SIZE // 2]]\n",
    "    \n",
    "    # Imprimir cada 10 generaciones\n",
    "    if (gen + 1) % 10 == 0 or gen == 0 or gen == N_GENERATIONS - 1:\n",
    "        print(f\"Generación {gen+1}: Mejor accuracy = {ranked[0][1]:.4f}\")\n",
    "    \n",
    "    next_pop = survivors.copy()\n",
    "    while len(next_pop) < POP_SIZE:\n",
    "        a, b = random.sample(survivors, 2)\n",
    "        c1, c2 = crossover(a, b)\n",
    "        next_pop += [mutate(c1), mutate(c2)]\n",
    "    pop = next_pop[:POP_SIZE]\n",
    "\n",
    "\n",
    "# 10) Mejor solución final\n",
    "final_scores = [fitness(ind) for ind in pop]\n",
    "best_idx = np.argmax(final_scores)\n",
    "best_ind = pop[best_idx]\n",
    "best_acc = final_scores[best_idx]\n",
    "best_features = X.columns[best_ind == 1].tolist()\n",
    "\n",
    "print(\"\\n Mejor Accuracy Promedio GA:\", best_acc)\n",
    "print(\" Columnas Seleccionadas:\", best_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e06ed5c",
   "metadata": {},
   "source": [
    "# **RED RNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797aa1a0",
   "metadata": {},
   "source": [
    "## Construcción de Secuencias para RNN\n",
    "\n",
    "Con el objetivo de entrenar una Red Neuronal Recurrente (RNN), transformamos nuestro dataset en una estructura de secuencias temporales. Cada secuencia agrupa **5 días consecutivos** de datos de una misma acción (`tic`) y se utiliza para predecir si el **último día** de la secuencia tiene una **alta volatilidad** (`volatilidad_alta = 1`) o no.\n",
    "\n",
    "### Proceso realizado:\n",
    "\n",
    "- Se generaron las secuencias **por acción**, respetando el orden cronológico de cada una.\n",
    "- Cada secuencia contiene los valores de los indicadores técnicos y precios durante 5 días.\n",
    "- La etiqueta (`y`) asociada a cada secuencia corresponde al valor de `volatilidad_alta` del día inmediatamente **posterior** a la secuencia.\n",
    "- Se excluyeron las columnas `date`, `tic` y `volatilidad_alta` del input `X`, ya que no son características útiles como entrada directa a la red.\n",
    "\n",
    "### Resultados obtenidos:\n",
    "\n",
    "- Total de secuencias generadas: **1686**\n",
    "- Dimensiones de `X_rnn`: `(1686, 5, 18)`\n",
    "  - 1686 ejemplos\n",
    "  - 5 pasos de tiempo por secuencia\n",
    "  - 18 características por paso de tiempo\n",
    "- Dimensiones de `y_rnn`: `(1686,)`\n",
    "  - Cada etiqueta es `0` o `1`, indicando si el día siguiente a la secuencia tuvo alta volatilidad.\n",
    "\n",
    "Este formato es el ideal para alimentar una arquitectura LSTM, GRU o RNN simple en frameworks como PyTorch o TensorFlow/Keras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2e97cb",
   "metadata": {},
   "source": [
    "## Ejemplo de cómo se construyen las secuencias para una RNN\n",
    "\n",
    "Cuando usamos una **RNN (Red Neuronal Recurrente)**, no alimentamos datos individuales, sino **secuencias de días consecutivos**. En este caso, usamos una **ventana de 5 días** (`sequence_length = 5`) y queremos predecir si el **día siguiente tendrá alta volatilidad** (`volatilidad_alta = 1`).\n",
    "\n",
    "---\n",
    "\n",
    "### Datos originales (simplificados)\n",
    "\n",
    "```text\n",
    "date        close   RSI   volumen   volatilidad_alta\n",
    "2024-01-01   100     30    1.5M           0\n",
    "2024-01-02   102     35    1.6M           0\n",
    "2024-01-03   105     40    1.4M           1\n",
    "2024-01-04   103     38    1.7M           0\n",
    "2024-01-05   106     45    1.6M           1\n",
    "2024-01-06   107     46    1.8M           0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Construcción de secuencias\n",
    "\n",
    "Con `sequence_length = 5`, generamos una **secuencia de entrada** (`X[0]`) a partir de los primeros 5 días:\n",
    "\n",
    "```python\n",
    "X[0] = [\n",
    "    [100, 30, 1.5M],\n",
    "    [102, 35, 1.6M],\n",
    "    [105, 40, 1.4M],\n",
    "    [103, 38, 1.7M],\n",
    "    [106, 45, 1.6M]\n",
    "]\n",
    "```\n",
    "\n",
    "La **etiqueta** correspondiente (`y[0]`) será el valor de `volatilidad_alta` del **día siguiente** (2024-01-06):\n",
    "\n",
    "```python\n",
    "y[0] = 0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ¿Cómo se genera el resto del dataset?\n",
    "\n",
    "Este proceso se repite de manera **deslizante** a lo largo del dataset para generar muchas secuencias con sus respectivas etiquetas.\n",
    "\n",
    "---\n",
    "\n",
    "### Formato final del conjunto de datos\n",
    "\n",
    "Cada entrada tiene forma:\n",
    "\n",
    "```text\n",
    "(secuencia, características) = (5, 18)\n",
    "```\n",
    "\n",
    "Donde:\n",
    "\n",
    "- `5` = número de días en la ventana,\n",
    "- `18` = número de características financieras por día.\n",
    "\n",
    "---\n",
    "\n",
    "Este formato es ideal para modelos de series de tiempo como **RNNs**, **LSTMs** o **GRUs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "992629f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danirm/.local/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.6587 - loss: 0.6515 - val_accuracy: 0.7515 - val_loss: 0.5772\n",
      "Epoch 2/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7311 - loss: 0.5821 - val_accuracy: 0.7426 - val_loss: 0.5729\n",
      "Epoch 3/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7587 - loss: 0.5469 - val_accuracy: 0.7485 - val_loss: 0.5821\n",
      "Epoch 4/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7584 - loss: 0.5375 - val_accuracy: 0.7189 - val_loss: 0.5810\n",
      "Epoch 5/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7671 - loss: 0.5240 - val_accuracy: 0.7160 - val_loss: 0.5759\n",
      "Epoch 6/30\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7722 - loss: 0.5137 - val_accuracy: 0.7249 - val_loss: 0.5850\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7918 - loss: 0.5204 \n",
      "Precisión en el conjunto de prueba: 0.7426\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ========================\n",
    "# 1. Preprocesamiento\n",
    "# ========================\n",
    "\n",
    "# Elimina columnas no deseadas\n",
    "df_model = processed_full.drop(columns=['return', 'volatility_5d'])\n",
    "\n",
    "# Aplica One-Hot Encoding a 'day' si está presente\n",
    "if 'day' in df_model.columns:\n",
    "    df_model = pd.get_dummies(df_model, columns=['day'], prefix='day')\n",
    "\n",
    "# Escalar todas las columnas numéricas excepto 'volatilidad_alta'\n",
    "cols_excluir = ['date', 'tic', 'volatilidad_alta']\n",
    "features = [col for col in df_model.columns if col not in cols_excluir]\n",
    "scaler_rnn = StandardScaler()\n",
    "df_model[features] = scaler_rnn.fit_transform(df_model[features])\n",
    "\n",
    "# ========================\n",
    "# 2. Construcción de Secuencias\n",
    "# ========================\n",
    "\n",
    "def construir_secuencias_por_accion(df, sequence_length=5):\n",
    "    X_seq, y_seq = [], []\n",
    "    tics = df['tic'].unique()\n",
    "    columnas_excluir = ['date', 'tic', 'volatilidad_alta']\n",
    "    feature_cols = [col for col in df.columns if col not in columnas_excluir]\n",
    "\n",
    "    for tic in tics:\n",
    "        df_tic = df[df['tic'] == tic].sort_values('date').reset_index(drop=True)\n",
    "        for i in range(sequence_length, len(df_tic)):\n",
    "            secuencia = df_tic.loc[i-sequence_length:i-1, feature_cols].values\n",
    "            etiqueta = df_tic.loc[i, 'volatilidad_alta']\n",
    "            X_seq.append(secuencia)\n",
    "            y_seq.append(etiqueta)\n",
    "\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Genera las secuencias\n",
    "X_rnn, y_rnn = construir_secuencias_por_accion(df_model, sequence_length=5)\n",
    "\n",
    "# ========================\n",
    "# 3. División de datos\n",
    "# ========================\n",
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(\n",
    "    X_rnn, y_rnn, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# Definimos el modelo mejorado\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, input_shape=(X_train_rnn.shape[1], X_train_rnn.shape[2])))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compilamos el modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "\n",
    "# Entrenamiento\n",
    "history = model.fit(\n",
    "    X_train_rnn, y_train_rnn,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_rnn, y_test_rnn),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluación\n",
    "loss, accuracy = model.evaluate(X_test_rnn, y_test_rnn)\n",
    "print(f\"Precisión en el conjunto de prueba: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e08c2948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.97      0.85       254\n",
      "           1       0.38      0.06      0.10        84\n",
      "\n",
      "    accuracy                           0.74       338\n",
      "   macro avg       0.57      0.51      0.48       338\n",
      "weighted avg       0.66      0.74      0.66       338\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_probs = model.predict(X_test_rnn)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "print(classification_report(y_test_rnn, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0a8d9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25504151838671413\n"
     ]
    }
   ],
   "source": [
    "print(y_rnn.mean())  # Si está cerca de 0.25, confirma el 25/75 de etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d190f1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos de clase: {0: 0.6726546906187625, 1: 1.9479768786127167}\n",
      "Epoch 1/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7843 - loss: 0.6587 - val_accuracy: 0.6183 - val_loss: 0.6687\n",
      "Epoch 2/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6110 - loss: 0.6637 - val_accuracy: 0.6302 - val_loss: 0.6798\n",
      "Epoch 3/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6529 - loss: 0.6451 - val_accuracy: 0.6420 - val_loss: 0.6968\n",
      "Epoch 4/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6606 - loss: 0.6279 - val_accuracy: 0.6479 - val_loss: 0.7010\n",
      "Epoch 5/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6696 - loss: 0.6179 - val_accuracy: 0.6331 - val_loss: 0.7218\n",
      "Epoch 6/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6959 - loss: 0.6066 - val_accuracy: 0.6391 - val_loss: 0.7095\n",
      "Epoch 7/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6647 - loss: 0.5939 - val_accuracy: 0.6450 - val_loss: 0.6919\n",
      "Epoch 8/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7216 - loss: 0.5807 - val_accuracy: 0.6213 - val_loss: 0.7198\n",
      "Epoch 9/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6840 - loss: 0.5679 - val_accuracy: 0.6391 - val_loss: 0.6830\n",
      "Epoch 10/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6981 - loss: 0.5549 - val_accuracy: 0.6065 - val_loss: 0.6968\n",
      "Epoch 11/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6783 - loss: 0.5641 - val_accuracy: 0.6302 - val_loss: 0.7009\n",
      "Epoch 12/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7123 - loss: 0.5544 - val_accuracy: 0.6095 - val_loss: 0.7345\n",
      "Epoch 13/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7027 - loss: 0.5494 - val_accuracy: 0.6124 - val_loss: 0.7013\n",
      "Epoch 14/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6683 - loss: 0.5521 - val_accuracy: 0.6657 - val_loss: 0.6925\n",
      "Epoch 15/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7139 - loss: 0.5453 - val_accuracy: 0.6716 - val_loss: 0.6839\n",
      "Epoch 16/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7564 - loss: 0.5343 - val_accuracy: 0.6272 - val_loss: 0.7138\n",
      "Epoch 17/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7308 - loss: 0.5199 - val_accuracy: 0.6716 - val_loss: 0.7223\n",
      "Epoch 18/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7350 - loss: 0.4897 - val_accuracy: 0.6331 - val_loss: 0.7267\n",
      "Epoch 19/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7697 - loss: 0.4863 - val_accuracy: 0.5651 - val_loss: 0.8202\n",
      "Epoch 20/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7395 - loss: 0.4928 - val_accuracy: 0.6154 - val_loss: 0.7689\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Calcular los pesos automáticamente\n",
    "pesos = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_rnn),\n",
    "    y=y_train_rnn\n",
    ")\n",
    "\n",
    "# Convertir a diccionario\n",
    "class_weight_dict = dict(enumerate(pesos))\n",
    "print(\"Pesos de clase:\", class_weight_dict)\n",
    "\n",
    "# Reentrenar con pesos\n",
    "history = model.fit(\n",
    "    X_train_rnn, y_train_rnn,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_rnn, y_test_rnn),\n",
    "    class_weight=class_weight_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8437bf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.66      0.72       254\n",
      "           1       0.32      0.48      0.38        84\n",
      "\n",
      "    accuracy                           0.62       338\n",
      "   macro avg       0.55      0.57      0.55       338\n",
      "weighted avg       0.67      0.62      0.64       338\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_probs = model.predict(X_test_rnn)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "print(classification_report(y_test_rnn, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "897b5387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentil 60: proporción de alta volatilidad = 0.399\n",
      "Percentil 65: proporción de alta volatilidad = 0.350\n",
      "Percentil 70: proporción de alta volatilidad = 0.301\n",
      "Percentil 75: proporción de alta volatilidad = 0.252\n",
      "Percentil 80: proporción de alta volatilidad = 0.199\n",
      "Percentil 85: proporción de alta volatilidad = 0.150\n",
      "Percentil 90: proporción de alta volatilidad = 0.101\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "percentiles = [60, 65, 70, 75, 80, 85, 90]\n",
    "\n",
    "for p in percentiles:\n",
    "    labels = processed_full.groupby('tic')['volatility_5d'].transform(lambda x: (x > np.percentile(x, p)).astype(int))\n",
    "    ratio = labels.mean()\n",
    "    print(f\"Percentil {p}: proporción de alta volatilidad = {ratio:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6c2538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tesis_maestria)",
   "language": "python",
   "name": "tesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
